{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un sistema de procesamiento en real time de tweets en castellano usando Python y Pyspark.\n",
    "\n",
    " \n",
    "\n",
    "-***(CALLER) Crear un programa Python que se conectevía socket a la API de Twitter (https://stream.twitter.com) y que descargue enreal time datos sobre algunas # de interés en castellano, enviar los datosrecibidos al Spark Streamming Server (2 puntos)***\n",
    "\n",
    "**(SERVER) Crear una aplicación Spark StreammingServer para recibir y procesar los datos recibidos en tiempo real (2 puntos)**\n",
    "\n",
    "***(SQL) Guardar los datos sumarizados de SparkStreamming en una base de dados SQL, ejemplo Sqilite (1 punto)***\n",
    "\n",
    "***Traducir del castellano al inglés usando alguna API de Python vía Spark en el SERVER con los Tweets recibidos del CALLER (1punto)***\n",
    "\n",
    "Aplicar análisis de sentimiento como vaderSentiment de Python via Spark en el SERVER con los Tweets recibidos del CALLER (1 punto)\n",
    "\n",
    "Aplicar algún modelo de Machine Learning a los datos (hasta 3 puntos, porque elimina la necesidad de Vader - apartado 5 - y imagino que lo hariais en castellano en ML, por lo que también elimina la necesidad del apartado 4)\n",
    "\n",
    "Calidad de la Presentación y Exposión, enseñar el sistema ejecutando en directo / valoraré si se ha hecho un dashboard para la presentación de los datos del SQL (2 puntos)\n",
    "\n",
    "\n",
    "Ejemplo vaderSentiment:\n",
    "\n",
    "vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "polarity_scores(\"The only reason the U.S. has reported onemillion cases of CoronaVirus is that our Testing is sooo much better than anyother country in the World. Other countries are way behind us in Testing, andtherefore show far fewer cases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-05-16 13:16:08 -----------\n",
      "----------- 2021-05-16 13:16:10 -----------\n",
      "----------- 2021-05-16 13:16:12 -----------\n",
      "----------- 2021-05-16 13:16:14 -----------\n"
     ]
    }
   ],
   "source": [
    "!pip install google_trans_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import traceback\n",
    "import json\n",
    "from pyspark.sql.session import SparkSession\n",
    "import os\n",
    "from google_trans_new import google_translator  \n",
    "import datetime\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set(\"spark.executor.extraClassPath\",  os.path.join(os.getcwd(), 'extras/sqlite-jdbc-3.34.0.jar'))\n",
    "conf.set(\"spark.driver.extraClassPath\", os.path.join(os.getcwd(), 'extras/sqlite-jdbc-3.34.0.jar'))\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"TwitterStreamApp\", conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "translator = google_translator()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'jdbc:sqlite:' + os.path.join(os.getcwd(), 'database.sql')\n",
    "\n",
    "df = spark.read.jdbc(url, 'Tweet')\n",
    "df.createOrReplaceTempView('Tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME='/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "!export PATH=$JAVA_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(tweet):\n",
    "  \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "  \n",
    "    sentiment = sid.polarity_scores(tweet)\n",
    "      \n",
    "    if sentiment['compound'] >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif sentiment['compound'] <= - 0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-05-16 13:15:12 -----------\n",
      "----------- 2021-05-16 13:15:14 -----------\n",
      "----------- 2021-05-16 13:15:16 -----------\n",
      "----------- 2021-05-16 13:15:18 -----------\n",
      "----------- 2021-05-16 13:15:20 -----------\n",
      "----------- 2021-05-16 13:15:22 -----------\n"
     ]
    }
   ],
   "source": [
    "#def aggregate_tags_count(new_values, total_sum):\n",
    "    #return sum(new_values) + (total_sum or 0)\n",
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        # obtén el contexto spark sql singleton desde el contexto actual\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        # convierte el RDD a Row RDD\n",
    "        row_rdd = rdd.map(lambda x: (x, translator.translate(\n",
    "                x['extended_tweet']['full_text'] if x['truncated'] else x['text'], lang_src='es', lang_tgt='en'\n",
    "            ))).map(lambda x: Row(\n",
    "            name=x[0]['user']['screen_name'],\n",
    "            date=datetime.datetime.fromtimestamp(int(x[0]['timestamp_ms']) / 1000).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            text=x[0]['extended_tweet']['full_text'] if x[0]['truncated'] else x[0]['text'],\n",
    "            transtext=x[1],\n",
    "            sentiment=sentiment_score(x[1])\n",
    "        ))\n",
    "        #row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        # crea un DF desde el Row RDD\n",
    "        tweets_df = sql_context.createDataFrame(row_rdd)\n",
    "        tweets_df.write.mode(\"append\").jdbc(url, 'Tweet')\n",
    "        # Registra el marco de data como tabla\n",
    "        #hashtags_df.registerTempTable(\"hashtags\")\n",
    "        \n",
    "        # obtén los 10 mejores hashtags de la tabla utilizando SQL e imprímelos\n",
    "        #hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "        #hashtag_counts_df.show()\n",
    "        \n",
    "        # llama a este método para preparar los 10 mejores hashtags DF y envíalos\n",
    "        #print(\"seve\")\n",
    "        #send_df_to_dashboard(hashtag_counts_df)\n",
    "    except:\n",
    "        #traceback.print_exception(type(ex), ex, ex.__traceback__)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- 2021-05-16 13:19:14 -----------\n",
      "----------- 2021-05-16 13:19:16 -----------\n",
      "----------- 2021-05-16 13:19:18 -----------\n",
      "----------- 2021-05-16 13:19:20 -----------\n",
      "----------- 2021-05-16 13:19:22 -----------\n",
      "----------- 2021-05-16 13:19:24 -----------\n",
      "----------- 2021-05-16 13:19:26 -----------\n",
      "----------- 2021-05-16 13:19:28 -----------\n",
      "----------- 2021-05-16 13:19:30 -----------\n",
      "----------- 2021-05-16 13:19:32 -----------\n",
      "----------- 2021-05-16 13:19:34 -----------\n",
      "----------- 2021-05-16 13:19:36 -----------\n",
      "----------- 2021-05-16 13:19:38 -----------\n",
      "----------- 2021-05-16 13:19:40 -----------\n",
      "----------- 2021-05-16 13:19:42 -----------\n",
      "----------- 2021-05-16 13:19:44 -----------\n",
      "----------- 2021-05-16 13:19:46 -----------\n",
      "----------- 2021-05-16 13:19:48 -----------\n",
      "----------- 2021-05-16 13:19:50 -----------\n",
      "----------- 2021-05-16 13:19:52 -----------\n",
      "----------- 2021-05-16 13:19:54 -----------\n",
      "----------- 2021-05-16 13:19:56 -----------\n",
      "----------- 2021-05-16 13:19:58 -----------\n",
      "----------- 2021-05-16 13:20:00 -----------\n",
      "----------- 2021-05-16 13:20:02 -----------\n",
      "----------- 2021-05-16 13:20:04 -----------\n",
      "----------- 2021-05-16 13:20:06 -----------\n",
      "----------- 2021-05-16 13:20:08 -----------\n",
      "----------- 2021-05-16 13:20:10 -----------\n",
      "----------- 2021-05-16 13:20:12 -----------\n",
      "----------- 2021-05-16 13:20:14 -----------\n",
      "----------- 2021-05-16 13:20:16 -----------\n",
      "----------- 2021-05-16 13:20:18 -----------\n",
      "----------- 2021-05-16 13:20:20 -----------\n",
      "----------- 2021-05-16 13:20:22 -----------\n",
      "----------- 2021-05-16 13:20:24 -----------\n",
      "----------- 2021-05-16 13:20:26 -----------\n",
      "----------- 2021-05-16 13:20:28 -----------\n",
      "----------- 2021-05-16 13:20:30 -----------\n",
      "----------- 2021-05-16 13:20:32 -----------\n",
      "----------- 2021-05-16 13:20:34 -----------\n",
      "----------- 2021-05-16 13:20:36 -----------\n",
      "----------- 2021-05-16 13:20:38 -----------\n",
      "----------- 2021-05-16 13:20:40 -----------\n",
      "----------- 2021-05-16 13:20:42 -----------\n",
      "----------- 2021-05-16 13:20:44 -----------\n",
      "----------- 2021-05-16 13:20:46 -----------\n",
      "----------- 2021-05-16 13:20:48 -----------\n",
      "----------- 2021-05-16 13:20:50 -----------\n",
      "----------- 2021-05-16 13:20:52 -----------\n",
      "----------- 2021-05-16 13:20:54 -----------\n",
      "----------- 2021-05-16 13:20:56 -----------\n",
      "----------- 2021-05-16 13:20:58 -----------\n",
      "----------- 2021-05-16 13:21:00 -----------\n",
      "----------- 2021-05-16 13:21:02 -----------\n",
      "----------- 2021-05-16 13:21:04 -----------\n",
      "----------- 2021-05-16 13:21:06 -----------\n",
      "----------- 2021-05-16 13:21:08 -----------\n",
      "----------- 2021-05-16 13:21:10 -----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# crea un contexto spark con la configuración anterior\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# crea el Contexto Streaming desde el contexto spark visto arriba con intervalo de 2 segundos\n",
    "ssc = StreamingContext(sc, 2)\n",
    "# establece un punto de control para permitir la recuperación de RDD\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "# lee data del puerto 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009)\n",
    "# divide cada Tweet en palabras\n",
    "#words = dataStream.flatMap(lambda line: line.split(\" \")).map(lambda x: (x, 1))\n",
    "tweets = dataStream.map(lambda x: json.loads(x))\n",
    "#words.pprint()\n",
    "#words.foreachRDD(process_rdd)\n",
    "\n",
    "# agrega la cuenta de cada hashtag a su última cuenta\n",
    "#words_totals = words.updateStateByKey(aggregate_tags_count)\n",
    "# procesa cada RDD generado en cada intervalo\n",
    "tweets.foreachRDD(process_rdd)\n",
    "mySsc = ssc\n",
    "\n",
    "# comienza la computación de streaming\n",
    "ssc.start()\n",
    "# espera que la transmisión termine\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
