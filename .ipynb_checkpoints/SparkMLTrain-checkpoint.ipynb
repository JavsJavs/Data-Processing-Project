{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import udf, size, col\n",
    "from pyspark.sql.types import (IntegerType, StringType, \n",
    "                               TimestampType, StructType,\n",
    "                               StructField, ArrayType,\n",
    "                               TimestampType)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StopWordsRemover, RegexTokenizer, \\\n",
    "        CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "#conf.set(\"spark.executor.extraClassPath\",  os.path.join(os.getcwd(), 'extras/sqlite-jdbc-3.34.0.jar'))\n",
    "#conf.set(\"spark.driver.extraClassPath\", os.path.join(os.getcwd(), 'extras/sqlite-jdbc-3.34.0.jar'))\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"TwitterStreamApp\", conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'jdbc:sqlite:' + os.path.join(os.getcwd(), 'database.sqlite')\n",
    "\n",
    "# Importación de los datos de entrenamiento\n",
    "\n",
    "schema = StructType([StructField(\"sentiment\", StringType()),\n",
    "                   StructField(\"id\", StringType()),\n",
    "                   StructField(\"date\", StringType()),\n",
    "                   StructField(\"flag\", StringType()),\n",
    "                   StructField(\"user\", StringType()),\n",
    "                   StructField(\"transtext\", StringType())\n",
    "                  ])\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.format('csv').schema(schema).load('datasets/tweets_clean.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"sentiment\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de los datos para entrenamiento y tests\n",
    "\n",
    "(trainingData, testData) = df.randomSplit([0.99, 0.01], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un PipelinedModel\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"transtext\", outputCol=\"words\")\n",
    "#stopwords = StopWordsRemover(inputCol=\"words\", outputCol='tokens')\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "train_df = pipelineFit.transform(trainingData)\n",
    "val_df = pipelineFit.transform(testData)\n",
    "train_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión logística para casificar los tweets\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy(\"prediction\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de los resultados\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup de los modelos\n",
    "\n",
    "pipelineFit.save('models/final_idf.model')\n",
    "lrModel.save('models/final_lr.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
